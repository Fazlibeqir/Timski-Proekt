{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xMmb0iWzIvgL7opA3NgEbRGNoInKXqor",
      "authorship_tag": "ABX9TyOVVDLM5dRCJ4vKyt3LeddW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fazlibeqir/Timski-Proekt/blob/main/fetch_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "jOxhcaqpzwMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LFiM1zZIzVWf"
      },
      "outputs": [],
      "source": [
        "# Install required libraries (only once in Colab)\n",
        "!pip install -q gdown pygbif pillow requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2p1bWO5O33Ct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8582e29c-72a1-4dd8-e5b1-0a1d2353268e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "4_puQudD031O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ebehDJLG3cMT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link utils\n",
        "module_path = '/content/drive/MyDrive/Insect Detection/utils'\n",
        "sys.path.append(module_path)\n",
        "\n",
        "from common_funcs import (\n",
        "    get_gbif_images,\n",
        "    download_image,\n",
        "    folder_has_enough_images,\n",
        "    count_images\n",
        ")"
      ],
      "metadata": {
        "id": "gbtT3T3q-rYQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and unzip dataset from Google Drive\n",
        "\n",
        "https://drive.google.com/file/d/17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA"
      ],
      "metadata": {
        "id": "1P2OiBUF057C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== OPTION 1: Load dataset from Google Drive ZIP =====\n",
        "load_from_zip = True  # ðŸ‘ˆ Set to False if you want to download via GBIF\n",
        "\n",
        "dataset_dir = \"/content/dataset/dataset/content/dataset\"  # Final dataset path\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "if load_from_zip:\n",
        "    print(\"ðŸ“¦ Loading dataset from Drive ZIP\")\n",
        "    file_id = \"17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA\"\n",
        "    output_path = \"dataset.zip\"\n",
        "\n",
        "    # Download from Google Drive\n",
        "    import gdown\n",
        "    gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "\n",
        "    # Extract if valid\n",
        "    if output_path.endswith(\".zip\") and zipfile.is_zipfile(output_path):\n",
        "        shutil.unpack_archive(output_path, \"dataset\", 'zip')\n",
        "        print(\"âœ… Dataset extracted successfully.\")\n",
        "    else:\n",
        "        print(\"âŒ Downloaded file is not a valid ZIP archive.\")\n",
        "\n",
        "else:\n",
        "    print(\"ðŸŒ Downloading dataset from GBIF using CSV class list\")\n",
        "\n",
        "    # ====== Load class list ======\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/csv/0044847-241126133413365.csv\")\n",
        "    classes = df[\"Scientific name\"].tolist()\n",
        "\n",
        "    # ====== Download Images from GBIF ======\n",
        "    min_images_per_class = 50\n",
        "    for cls in tqdm(classes, desc=\"Downloading images\"):\n",
        "        folder_name = cls.replace(\" \", \"_\")\n",
        "        class_folder = os.path.join(dataset_dir, folder_name)\n",
        "        os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "        urls = get_gbif_images(cls, limit=200)\n",
        "        count = 0\n",
        "        for i, url in enumerate(urls):\n",
        "            filename = f\"{i}.jpg\"\n",
        "            save_path = os.path.join(class_folder, filename)\n",
        "            success = download_image(url, save_path)\n",
        "            if success:\n",
        "                count += 1\n",
        "            if count >= min_images_per_class:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "slGLVNdh01vE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ab0866-9009-4dab-9c73-1b7af2067dd5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Loading dataset from Drive ZIP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA\n",
            "From (redirected): https://drive.google.com/uc?id=17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA&confirm=t&uuid=d30648f1-e274-49ba-8ce3-bd23af7b5644\n",
            "To: /content/dataset.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42G/4.42G [01:20<00:00, 54.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean corrupted images\n",
        "\n",
        "Opens every image to verify it's not corrupted.\n",
        "\n",
        "Deletes unreadable or broken images."
      ],
      "metadata": {
        "id": "qozDbJJE1VHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"ðŸ§¹ Cleaning corrupted images in:\", dataset_dir)\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "     split_dir = os.path.join(dataset_dir, split)\n",
        "     if not os.path.isdir(split_dir):\n",
        "         continue\n",
        "\n",
        "     for class_folder in os.listdir(split_dir):\n",
        "         class_path = os.path.join(split_dir, class_folder)\n",
        "         if not os.path.isdir(class_path):\n",
        "             continue\n",
        "\n",
        "         for file in os.listdir(class_path):\n",
        "             file_path = os.path.join(class_path, file)\n",
        "\n",
        "             # Skip non-files\n",
        "             if not os.path.isfile(file_path):\n",
        "                 continue\n",
        "\n",
        "             try:\n",
        "                 with Image.open(file_path) as img:\n",
        "                     img.verify()\n",
        "             except:\n",
        "                 os.remove(file_path)\n",
        "                 print(f\"âŒ Removed corrupted image: {file_path}\")\n",
        "\n",
        "print(\"âœ… Done cleaning corrupted images.\")\n"
      ],
      "metadata": {
        "id": "I2eS7CFJ1Lac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513dcbcb-3908-475a-94b6-f0072f5edf58"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¹ Cleaning corrupted images in: /content/dataset/dataset/content/dataset\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_8.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_7.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_0.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_9.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_6.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_2.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_12.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_10.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_13.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_4.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_11.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_3.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/val/Solenopsis_pollux_Forel,_1893/image_1.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/val/Solenopsis_pollux_Forel,_1893/image_0.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/test/Solenopsis_pollux_Forel,_1893/image_1.jpg\n",
            "âŒ Removed corrupted image: /content/dataset/dataset/content/dataset/test/Solenopsis_pollux_Forel,_1893/image_2.jpg\n",
            "âœ… Done cleaning corrupted images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out classes with too few images\n",
        "\n",
        "Keeps only classes with enough valid images.\n",
        "\n",
        "Deletes folders with too few images to ensure training quality."
      ],
      "metadata": {
        "id": "Wtgbfl3T1PfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"ðŸ” Filtering classes with too few images based on 'train/' split...\")\n",
        "min_images=5\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "valid_classes = []\n",
        "\n",
        "# Check each class folder in 'train'\n",
        "for class_folder in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_folder)\n",
        "\n",
        "    if folder_has_enough_images(class_path, min_images):\n",
        "        valid_classes.append(class_folder)\n",
        "    else:\n",
        "        print(f\"âŒ Removing class '{class_folder}' (not enough images)\")\n",
        "        # Remove class folder from all splits\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            split_path = os.path.join(dataset_dir, split, class_folder)\n",
        "            shutil.rmtree(split_path, ignore_errors=True)\n",
        "\n",
        "print(f\"âœ… Valid classes remaining: {len(valid_classes)}\")"
      ],
      "metadata": {
        "id": "KR3iQYuD1BIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539ce119-6619-47af-f093-3edc35be183b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Filtering classes with too few images based on 'train/' split...\n",
            "âŒ Removing class 'Bombus_defector_Skorikov,_1910' (not enough images)\n",
            "âŒ Removing class 'Bombus_hortorum_hortorum' (not enough images)\n",
            "âŒ Removing class 'Bombus_ruderatus_autumnalis_(Fabricius,_1793)' (not enough images)\n",
            "âŒ Removing class 'Bombus_pratorum_pratorum' (not enough images)\n",
            "âŒ Removing class 'Solenopsis_pollux_Forel,_1893' (not enough images)\n",
            "âŒ Removing class 'Bombus_jonellus_jonellus' (not enough images)\n",
            "âœ… Valid classes remaining: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create 'data.yaml' for YOLOv8 classificiation"
      ],
      "metadata": {
        "id": "4UJ8KrCj1bDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ“ Generating data.yaml...\")\n",
        "\n",
        "data_yaml_content = f\"\"\"\n",
        "path: {dataset_dir}\n",
        "train: .\n",
        "val: .\n",
        "test: .\n",
        "\n",
        "names:\n",
        "\"\"\"\n",
        "\n",
        "for idx, cls in enumerate(valid_classes):\n",
        "    class_name = cls.replace(\" \", \"_\")\n",
        "    data_yaml_content += f\"  {idx}: {class_name}\\n\"\n",
        "\n",
        "data_yaml_path = os.path.join(dataset_dir, \"data.yaml\")\n",
        "with open(data_yaml_path, \"w\") as f:\n",
        "    f.write(data_yaml_content.strip())\n",
        "\n",
        "print(\"âœ… data.yaml created!\")\n"
      ],
      "metadata": {
        "id": "44N8Bnl51U_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f596be-40af-4e0d-e6ee-57478c4f81d6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Generating data.yaml...\n",
            "âœ… data.yaml created!\n"
          ]
        }
      ]
    }
  ]
}