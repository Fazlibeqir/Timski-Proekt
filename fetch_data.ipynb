{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xMmb0iWzIvgL7opA3NgEbRGNoInKXqor",
      "authorship_tag": "ABX9TyOO37CNsW7yGUUwL+q3GVof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fazlibeqir/Timski-Proekt/blob/main/fetch_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "jOxhcaqpzwMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFiM1zZIzVWf"
      },
      "outputs": [],
      "source": [
        "# Install required libraries (only once in Colab)\n",
        "!pip install -q gdown pillow requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2p1bWO5O33Ct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10396c6-a285-4d1f-de80-ee447cad82b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "4_puQudD031O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ebehDJLG3cMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link utils\n",
        "module_path = '/content/drive/MyDrive/Insect Detection/utils'\n",
        "sys.path.append(module_path)\n",
        "\n",
        "from common_funcs import (\n",
        "    get_gbif_images,\n",
        "    download_image,\n",
        "    folder_has_enough_images,\n",
        "    count_images,\n",
        "    split_and_download_images\n",
        ")"
      ],
      "metadata": {
        "id": "gbtT3T3q-rYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and unzip dataset from Google Drive\n",
        "\n",
        "https://drive.google.com/file/d/17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA"
      ],
      "metadata": {
        "id": "1P2OiBUF057C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üéØ Configuration\n"
      ],
      "metadata": {
        "id": "z20whLouS5n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_from_zip = True  # üëà Set to False if you want to download via GBIF\n",
        "\n",
        "dataset_dir = \"/content/dataset/dataset/content/dataset\"  # Final dataset path\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "val_dir = os.path.join(dataset_dir, \"val\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "\n",
        "os.makedirs(dataset_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "N9KFvGYfSxLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# üì¶ Option 1: Load from ZIP\n",
        "# ===============================\n",
        "if load_from_zip:\n",
        "    print(\"üì¶ Loading dataset from Drive ZIP\")\n",
        "    file_id = \"17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA\"\n",
        "    output_path = \"dataset.zip\"\n",
        "\n",
        "    # Download from Google Drive\n",
        "    import gdown\n",
        "    gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "\n",
        "    # Extract if valid\n",
        "    if output_path.endswith(\".zip\") and zipfile.is_zipfile(output_path):\n",
        "        shutil.unpack_archive(output_path, \"dataset\", 'zip')\n",
        "        print(\"‚úÖ Dataset extracted successfully.\")\n",
        "    else:\n",
        "        print(\"‚ùå Downloaded file is not a valid ZIP archive.\")\n",
        "\n",
        "# ===============================\n",
        "# üåê Option 2: Download from GBIF\n",
        "# ===============================\n",
        "else:\n",
        "    print(\"üåê Downloading dataset from GBIF using CSV class list\")\n",
        "\n",
        "    # ====== Load class list ======\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/csv/0044847-241126133413365.csv\")\n",
        "    classes = df[\"Scientific name\"].tolist()\n",
        "\n",
        "    # ====== Download Images from GBIF ======\n",
        "    min_images_per_class = 30\n",
        "    max_images_per_class = 200\n",
        "\n",
        "    for cls in tqdm(classes, desc=\"Processing species\"):\n",
        "        urls = get_gbif_images(cls, limit=max_images_per_class)\n",
        "\n",
        "        if len(urls) < min_images_per_class:\n",
        "            print(f\"‚è≠Ô∏è Skipping '{cls}' (only {len(urls)} images)\")\n",
        "            continue\n",
        "\n",
        "        success = split_and_download_images(\n",
        "            cls, urls,\n",
        "            train_dir=train_dir,\n",
        "            val_dir=val_dir,\n",
        "            test_dir=test_dir,\n",
        "            max_images_per_species=max_images_per_class\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            print(f\"‚úÖ Processed '{cls}'\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to process '{cls}'\")\n"
      ],
      "metadata": {
        "id": "slGLVNdh01vE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1b4491-0b0c-45e5-fb52-405f62d3ca2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Loading dataset from Drive ZIP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA\n",
            "From (redirected): https://drive.google.com/uc?id=17F34dlZgpaYxy04nFqEQz2ffwMQUrfxA&confirm=t&uuid=f5bca1af-61b5-4327-98bf-05adff93b2d0\n",
            "To: /content/dataset.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42G/4.42G [00:51<00:00, 85.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean corrupted images\n",
        "\n",
        "Opens every image to verify it's not corrupted.\n",
        "\n",
        "Deletes unreadable or broken images."
      ],
      "metadata": {
        "id": "qozDbJJE1VHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"üßπ Cleaning corrupted images in:\", dataset_dir)\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "     split_dir = os.path.join(dataset_dir, split)\n",
        "     if not os.path.isdir(split_dir):\n",
        "         continue\n",
        "\n",
        "     for class_folder in os.listdir(split_dir):\n",
        "         class_path = os.path.join(split_dir, class_folder)\n",
        "         if not os.path.isdir(class_path):\n",
        "             continue\n",
        "\n",
        "         for file in os.listdir(class_path):\n",
        "             file_path = os.path.join(class_path, file)\n",
        "\n",
        "             # Skip non-files\n",
        "             if not os.path.isfile(file_path):\n",
        "                 continue\n",
        "\n",
        "             try:\n",
        "                 with Image.open(file_path) as img:\n",
        "                     img.verify()\n",
        "             except:\n",
        "                 os.remove(file_path)\n",
        "                 print(f\"‚ùå Removed corrupted image: {file_path}\")\n",
        "\n",
        "print(\"‚úÖ Done cleaning corrupted images.\")\n"
      ],
      "metadata": {
        "id": "I2eS7CFJ1Lac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e813fc5-87cf-477c-96e0-b42f8c5ee34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Cleaning corrupted images in: /content/dataset/dataset/content/dataset\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_12.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_6.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_0.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_4.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_13.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_7.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_3.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_8.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_10.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_11.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_9.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/train/Solenopsis_pollux_Forel,_1893/image_2.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/val/Solenopsis_pollux_Forel,_1893/image_0.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/val/Solenopsis_pollux_Forel,_1893/image_1.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/test/Solenopsis_pollux_Forel,_1893/image_1.jpg\n",
            "‚ùå Removed corrupted image: /content/dataset/dataset/content/dataset/test/Solenopsis_pollux_Forel,_1893/image_2.jpg\n",
            "‚úÖ Done cleaning corrupted images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out classes with too few images\n",
        "\n",
        "Keeps only classes with enough valid images.\n",
        "\n",
        "Deletes folders with too few images to ensure training quality."
      ],
      "metadata": {
        "id": "Wtgbfl3T1PfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"üîç Filtering classes with too few images based on 'train/' split...\")\n",
        "min_images=5\n",
        "valid_classes = []\n",
        "\n",
        "# Check each class folder in 'train'\n",
        "for class_folder in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_folder)\n",
        "\n",
        "    if folder_has_enough_images(class_path, min_images):\n",
        "        valid_classes.append(class_folder)\n",
        "    else:\n",
        "        print(f\"‚ùå Removing class '{class_folder}' (not enough images)\")\n",
        "        # Remove class folder from all splits\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            split_path = os.path.join(dataset_dir, split, class_folder)\n",
        "            shutil.rmtree(split_path, ignore_errors=True)\n",
        "\n",
        "print(f\"‚úÖ Valid classes remaining: {len(valid_classes)}\")"
      ],
      "metadata": {
        "id": "KR3iQYuD1BIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecd8169-2255-43f5-a29c-dba1d4e3fc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Filtering classes with too few images based on 'train/' split...\n",
            "‚ùå Removing class 'Bombus_ruderatus_autumnalis_(Fabricius,_1793)' (not enough images)\n",
            "‚ùå Removing class 'Bombus_pratorum_pratorum' (not enough images)\n",
            "‚ùå Removing class 'Bombus_jonellus_jonellus' (not enough images)\n",
            "‚ùå Removing class 'Bombus_defector_Skorikov,_1910' (not enough images)\n",
            "‚ùå Removing class 'Solenopsis_pollux_Forel,_1893' (not enough images)\n",
            "‚ùå Removing class 'Bombus_hortorum_hortorum' (not enough images)\n",
            "‚úÖ Valid classes remaining: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create 'data.yaml' for YOLOv8 classificiation"
      ],
      "metadata": {
        "id": "4UJ8KrCj1bDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìù Generating data.yaml...\")\n",
        "\n",
        "data_yaml_content = f\"\"\"\n",
        "path: {dataset_dir}\n",
        "train: {train_dir}\n",
        "val: {val_dir}\n",
        "test: {test_dir}\n",
        "\n",
        "names:\n",
        "\"\"\"\n",
        "\n",
        "for idx, cls in enumerate(valid_classes):\n",
        "    data_yaml_content += f\"  {idx}: {cls}\\n\"\n",
        "\n",
        "with open(os.path.join(dataset_dir, \"data.yaml\"), \"w\") as f:\n",
        "    f.write(data_yaml_content.strip())\n",
        "\n",
        "print(\"‚úÖ data.yaml created!\")\n"
      ],
      "metadata": {
        "id": "44N8Bnl51U_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdd672d-03e0-4e96-de5d-8bce419b5cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Generating data.yaml...\n",
            "‚úÖ data.yaml created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compress the cleaned dataset and move to Google Drive if mounted"
      ],
      "metadata": {
        "id": "K0KXHNMfUK3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive(\"cleaned_dataset\", 'zip', \"dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IjuFeJgJUPwt",
        "outputId": "1806a91c-cda4-4121-b2e0-2e87da4e9cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/cleaned_dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp cleaned_dataset.zip /content/drive/MyDrive/Insect\\ Detection/\n",
        "print(\"‚úÖ Cleaned dataset zipped and saved to Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1yEDvVBYSvm",
        "outputId": "24afdab7-a6ab-4d46-8011-0f61d38685f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned dataset zipped and saved to Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/Insect\\ Detection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWT8S0jkbH_w",
        "outputId": "a9713231-b069-45c6-a401-ba475d594662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.2G\n",
            "-rw------- 1 root root 4.2G May 30 18:03 cleaned_dataset.zip\n",
            "-rw------- 1 root root  324 May 29 23:17 compare_all_versions.ipynb\n",
            "-rw------- 1 root root  14K May 30 18:02 fetch_data.ipynb\n",
            "-rw------- 1 root root  324 May 29 23:17 test_and_eval.ipynb\n",
            "-rw------- 1 root root 1.6M May 30 17:59 train_v1_baseline.ipynb\n",
            "-rw------- 1 root root  324 May 29 23:16 train_v2_augmented.ipynb\n",
            "-rw------- 1 root root  324 May 29 23:16 train_v3_bigger_model.ipynb\n",
            "drwx------ 3 root root 4.0K May 28 16:56 utils\n",
            "-rw------- 1 root root 909K May 28 16:44 YOLO8.ipynb\n"
          ]
        }
      ]
    }
  ]
}